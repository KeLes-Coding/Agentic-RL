import os
import glob
import pandas as pd
import argparse
import logging
import random
import json
import hashlib

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def get_all_game_files(base_path, split_names=["train"]):
    """æ‰«ææŒ‡å®š split ç›®å½•ä¸‹çš„æ‰€æœ‰æ¸¸æˆæ–‡ä»¶"""
    all_files = []
    for split in split_names:
        search_pattern = os.path.join(base_path, split, "**", "game.tw-pddl")
        files = glob.glob(search_pattern, recursive=True)
        # logger.info(f"Scanned {split}: found {len(files)} files.")
        all_files.extend(files)
    return all_files

def process_files(file_list, split_label):
    data_list = []
    for f_path in file_list:
        full_dir = os.path.dirname(f_path)
        
        # æ„é€ ç›¸å¯¹è·¯å¾„ä½œä¸º ID
        try:
            rel_path = full_dir.split("json_2.1.1/")[-1] 
        except IndexError:
            rel_path = full_dir 

        # æ„é€ ç¬¦åˆ Chat Template çš„ prompt
        chat_prompt = [
            {
                "role": "user",
                "content": rel_path 
            }
        ]

        data_list.append({
            "prompt": chat_prompt,
            "prompt_index": rel_path,
            "game_path": full_dir,
            "ability": "alfworld",
            "split": split_label,
            "data_source": "alfworld"
        })
    return data_list

def extract_task_type(game_file_path: str, data_root: str) -> str:
    """
    ä» game æ–‡ä»¶è·¯å¾„ä¸­æå–ä»»åŠ¡ç±»å‹ï¼ˆtask typeï¼‰ã€‚
    çº¦å®šï¼šdata_root ä¸‹é€šå¸¸æ˜¯ <split>/<task_type>/.../game.tw-pddl
    """
    full_dir = os.path.dirname(game_file_path)
    try:
        rel_dir = os.path.relpath(full_dir, data_root)
    except Exception:
        rel_dir = full_dir

    parts = rel_dir.split(os.sep)
    if not parts:
        return "unknown"

    # å¸¸è§ç»“æ„ï¼štrain/<task_type>/...
    if parts[0] in {"train", "valid", "test"}:
        return parts[1] if len(parts) > 1 else "unknown"

    # å…œåº•ï¼šæ²¡æœ‰ split å‰ç¼€æ—¶ï¼Œå–ç¬¬ä¸€æ®µ
    return parts[0]

def balanced_sample(file_pool, k, rng, data_root):
    """
    å°½é‡æŒ‰ task_type å‡åŒ€æŠ½æ · k æ¡ï¼ˆround-robinï¼‰ï¼Œå¹¶ä¿æŒç¡®å®šæ€§ï¼ˆç”± rng æ§åˆ¶ï¼‰ã€‚
    è¿”å›ï¼šselected(list)
    """
    if k <= 0:
        return []

    task_to_files = {}
    for fp in file_pool:
        t = extract_task_type(fp, data_root)
        task_to_files.setdefault(t, []).append(fp)

    # æ¯ä¸ª task å†…éƒ¨æ‰“ä¹±
    for t in task_to_files:
        rng.shuffle(task_to_files[t])

    # task åˆ—è¡¨é¡ºåºä¹Ÿç”± rng å†³å®šï¼ˆåœ¨ seed å›ºå®šæ—¶ä¿æŒç¡®å®šæ€§ï¼‰
    tasks = sorted(task_to_files.keys())
    rng.shuffle(tasks)

    selected = []
    # round-robin æŠ½å–ï¼Œç›´åˆ°å‡‘å¤Ÿ k æˆ–è€…æ‰€æœ‰ task è€—å°½
    while len(selected) < k:
        progressed = False
        for t in tasks:
            if len(selected) >= k:
                break
            if task_to_files[t]:
                selected.append(task_to_files[t].pop())
                progressed = True
        if not progressed:
            break

    return selected

def get_config_hash(args):
    """
    è®¡ç®—é…ç½®çš„å“ˆå¸Œå€¼ã€‚
    å¿…é¡»åŒ…å«æ‰€æœ‰å½±å“æ•°æ®é›†åˆ’åˆ†çš„å‚æ•°ï¼Œç¡®ä¿ä»»ä½•å˜åŠ¨éƒ½èƒ½è§¦å‘é‡æ–°ç”Ÿæˆã€‚
    """
    config_dict = {
        "data_root": args.data_root,
        "seed": args.seed,
        # é‡‡æ ·ç­–ç•¥ï¼ˆæ–°å¢ï¼šå‡åŒ€æŒ‰ä»»åŠ¡ç±»å‹æŠ½æ ·ï¼‰
        "sampling_strategy": "balanced_by_task_type_round_robin_v1",
        # æ ¸å¿ƒï¼šå°†ä¸¤ç§æ¨¡å¼çš„å‚æ•°éƒ½æ”¾å…¥å­—å…¸
        "mode_params": {
            "total_samples": args.total_samples,
            "train_ratio": args.train_ratio,
            "explicit_train_size": args.train_size,  # å…³é”®ï¼šçº³å…¥æ˜¾å¼å¤§å°
            "explicit_val_size": args.val_size       # å…³é”®ï¼šçº³å…¥æ˜¾å¼å¤§å°
        }
    }
    # æŒ‰ç…§ Key æ’åºè½¬å­—ç¬¦ä¸²ï¼Œç¡®ä¿ç¡®å®šæ€§
    config_str = json.dumps(config_dict, sort_keys=True)
    return hashlib.md5(config_str.encode('utf-8')).hexdigest(), config_dict

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--data_root", type=str, default=os.path.expanduser("~/.cache/alfworld/json_2.1.1"))
    parser.add_argument("--output_dir", type=str, default="data/verl-agent/text")

    # === æ¨¡å¼ A: æ¯”ä¾‹é‡‡æ · (Legacy) ===
    parser.add_argument("--total_samples", type=int, default=-1, help="[Ratio Mode] æ€»æ•°æ®é‡ï¼Œ-1ä¸ºå…¨é‡")
    parser.add_argument("--train_ratio", type=float, default=0.8, help="[Ratio Mode] è®­ç»ƒé›†å æ¯”")

    # === æ¨¡å¼ B: æ˜¾å¼æŒ‡å®šå¤§å° (New) ===
    parser.add_argument("--train_size", type=int, default=0, help="[Explicit Mode] æ˜¾å¼æŒ‡å®šè®­ç»ƒé›†æ•°é‡")
    parser.add_argument("--val_size", type=int, default=0, help="[Explicit Mode] æ˜¾å¼æŒ‡å®šéªŒè¯é›†æ•°é‡")

    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    parser.add_argument("--force", action="store_true", help="å¼ºåˆ¶é‡æ–°ç”Ÿæˆ")

    args = parser.parse_args()

    # --- 1. æ£€æŸ¥ç¼“å­˜ (å†³å®šæ˜¯å¦è·³è¿‡) ---
    os.makedirs(args.output_dir, exist_ok=True)
    meta_path = os.path.join(args.output_dir, "dataset_meta.json")
    train_path = os.path.join(args.output_dir, "train.parquet")
    test_path = os.path.join(args.output_dir, "test.parquet")

    # è®¡ç®—å½“å‰çš„æŒ‡çº¹
    current_hash, current_config = get_config_hash(args)

    need_regenerate = True
    if os.path.exists(meta_path) and os.path.exists(train_path) and not args.force:
        try:
            with open(meta_path, 'r') as f:
                saved_meta = json.load(f)
            # å¯¹æ¯”æŒ‡çº¹
            if saved_meta.get('config_hash') == current_hash:
                logger.info(f"âœ… Config Check: Hash matched ({current_hash[:8]}). Using cached dataset.")
                need_regenerate = False
            else:
                logger.info(
                    f"âš ï¸  Config Check: Hash mismatch! (Saved: {saved_meta.get('config_hash')[:8]} vs Current: {current_hash[:8]}). Regenerating..."
                )
        except Exception as e:
            logger.warning(f"âš ï¸  Config Check: Error reading meta ({e}). Regenerating...")
    else:
        logger.info("â„¹ï¸  No cache found or force update. Generating...")

    if not need_regenerate:
        return

    # --- 2. å¼€å§‹ç”Ÿæˆæ•°æ® ---
    if not os.path.exists(args.data_root):
        raise FileNotFoundError(f"ALFWorld data not found at {args.data_root}")

    # æ‰«ææ–‡ä»¶
    logger.info("Scanning files...")
    raw_files = get_all_game_files(args.data_root, split_names=["train"])
    if not raw_files:
        logger.error("No game files found!")
        return

    # ä¸ºç¡®å®šæ€§ï¼šå…ˆæ’åºï¼Œå†ç”¨å›ºå®š seed çš„ rng æ§åˆ¶æŠ½æ ·è¿‡ç¨‹
    raw_files.sort()
    rng = random.Random(args.seed)
    rng.shuffle(raw_files)

    total_available = len(raw_files)

    train_files = []
    test_files = []

    # ================= æ ¸å¿ƒåˆ¤å®šé€»è¾‘ =================
    # åˆ¤å®šä¼˜å…ˆçº§ï¼šå¦‚æœ train_size æˆ– val_size è¢«è®¾ç½®(>0)ï¼Œåˆ™å¼ºåˆ¶è¿›å…¥ Explicit Mode
    if args.train_size > 0 or args.val_size > 0:
        logger.info(f"ğŸ”µ Mode: EXPLICIT SIZE (Train: {args.train_size}, Val: {args.val_size})")

        req_train = args.train_size
        req_val = args.val_size

        # è¾¹ç•Œæ£€æŸ¥
        if req_train + req_val > total_available:
            logger.warning(f"âš ï¸  Requested {req_train + req_val} > Available {total_available}. Truncating Train set first.")
            if req_val > total_available:
                req_val = total_available
                req_train = 0
            else:
                req_train = total_available - req_val

        # å…ˆå‡åŒ€æŠ½ trainï¼Œå†ä»å‰©ä½™é‡Œå‡åŒ€æŠ½ valï¼Œä¿è¯ä¸é‡å 
        train_files = balanced_sample(raw_files, req_train, rng, args.data_root)
        remaining = [fp for fp in raw_files if fp not in set(train_files)]
        test_files = balanced_sample(remaining, req_val, rng, args.data_root)

    else:
        logger.info(f"ğŸŸ£ Mode: RATIO SAMPLING (Total: {args.total_samples}, Ratio: {args.train_ratio})")

        num_to_take = args.total_samples
        if num_to_take == -1 or num_to_take > total_available:
            num_to_take = total_available

        num_train = int(num_to_take * args.train_ratio)
        if num_train == 0 and num_to_take > 0:
            num_train = 1  # è‡³å°‘ä¿è¯è®­ç»ƒé›†æœ‰1æ¡
        num_val = max(0, num_to_take - num_train)

        # åœ¨å…¨æ± é‡Œåšâ€œæŒ‰ä»»åŠ¡ç±»å‹å°½é‡å‡åŒ€â€çš„æŠ½æ ·ï¼šå…ˆ train å† valï¼ˆä¸é‡å ï¼‰
        train_files = balanced_sample(raw_files, num_train, rng, args.data_root)
        remaining = [fp for fp in raw_files if fp not in set(train_files)]
        test_files = balanced_sample(remaining, num_val, rng, args.data_root)
    # =================================================

    logger.info(f"Result -> Train: {len(train_files)} | Val: {len(test_files)}")

    # ä¿å­˜ Parquet
    df_train = pd.DataFrame(process_files(train_files, "train"))
    df_train.to_parquet(train_path)

    if test_files:
        df_test = pd.DataFrame(process_files(test_files, "test"))
        df_test.to_parquet(test_path)
    else:
        # å¦‚æœæ²¡æœ‰æµ‹è¯•é›†ï¼Œæ¸…ç†æ—§æ–‡ä»¶é˜²æ­¢æ··æ·†
        if os.path.exists(test_path):
            os.remove(test_path)

    # ä¿å­˜å…ƒæ•°æ® (æ›´æ–° Hash)
    meta_info = {
        "config_hash": current_hash,
        "config": current_config,
        "generated_at": pd.Timestamp.now().isoformat(),
        "stats": {
            "train_len": len(train_files),
            "val_len": len(test_files)
        }
    }
    with open(meta_path, 'w') as f:
        json.dump(meta_info, f, indent=2)

    logger.info(f"âœ… Dataset generated and saved to {args.output_dir}")

if __name__ == "__main__":
    main()